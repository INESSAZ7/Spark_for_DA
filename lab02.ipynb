{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 4 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"lab_asd\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = [23126, 21617, 16627, 11556, 16704, 13702]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\").select(\"id\", \"lang\", \"name\", F.lower(F.regexp_replace('desc', r'[^\\pL{0-9}\\p{Space}]', '')).alias('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(FloatType())\n",
    "def cos_sim(v, u):\n",
    "    return float(v.dot(u) / (v.norm(2) * u.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for course in courses:\n",
    "    course_lang = df.filter(F.col(\"id\") == course).collect()[0][\"lang\"]\n",
    "    if course_lang == \"en\":\n",
    "        stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")        \n",
    "    elif course_lang == \"es\":\n",
    "        stop_words = StopWordsRemover.loadDefaultStopWords(\"spanish\")\n",
    "    elif course_lang == \"ru\":\n",
    "        stop_words = StopWordsRemover.loadDefaultStopWords(\"russian\")   \n",
    "    \n",
    "    lang_df = df.filter(df.lang == course_lang)\n",
    "    tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "    swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered\", stopWords=stop_words)\n",
    "    count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"word_vector\", vocabSize=10000)\n",
    "    tfidf = IDF(inputCol=\"word_vector\", outputCol=\"tfidf\")\n",
    "    \n",
    "    preprocessing = Pipeline(stages=[\n",
    "        tokenizer,\n",
    "        swr,\n",
    "        count_vectorizer,\n",
    "        tfidf\n",
    "    ])\n",
    "    \n",
    "    preprocessing_model = preprocessing.fit(lang_df)\n",
    "    preprocessed_dataset = preprocessing_model.transform(lang_df)\n",
    "    \n",
    "    result = (\n",
    "        preprocessed_dataset\n",
    "        .crossJoin(preprocessed_dataset.filter(F.col(\"id\") == course).select(F.col(\"tfidf\").alias(\"vector\")))\n",
    "        .select('*', cos_sim('tfidf', 'vector').alias('cosine'))\n",
    "        .filter(\"cosine <> 'NaN'\")\n",
    "        .select('*', F.row_number().over(Window.partitionBy(\"vector\").orderBy(F.col('cosine').desc(), \n",
    "                                                                              F.col('name'),\n",
    "                                                                              F.col('id'))).alias('rn'))\n",
    "        .filter(\"rn <= 11 AND rn > 1\")\n",
    "        .orderBy(\"rn\")\n",
    "    )\n",
    "    \n",
    "    result_array = result.select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "    result_dict[course] = result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [14760, 13665, 13782, 15909, 19270, 25782, 13348, 17499, 25071, 7153],\n",
       " 21617: [21609, 21608, 21616, 21492, 21624, 21623, 21630, 21628, 21700, 21508],\n",
       " 16627: [11431, 5687, 12247, 17964, 5558, 16694, 17961, 12660, 9598, 11575],\n",
       " 11556: [16488, 13461, 11523, 468, 22710, 10447, 23357, 11529, 19330, 9465],\n",
       " 16704: [1228, 1327, 20362, 18331, 26980, 1365, 1247, 8186, 1236, 20645],\n",
       " 13702: [13702, 21079, 8123, 1041, 1396, 22053, 17076, 8082, 1033, 1052]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
