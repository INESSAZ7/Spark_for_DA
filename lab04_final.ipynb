{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding:0px !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"<style>#notebook { padding:0px !important; }</style>\"))\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 4 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\")\n",
    "         .appName(\"dont\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "Structure_Schema = StructType([\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('age', StringType(), True),\n",
    "    StructField('uid', StringType(), True),\n",
    "    StructField('user_json', MapType(StringType(), ArrayType(MapType(StringType(), StringType())), False), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data0 = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").option(\"inferSchema\", True).csv(\"/labs/slaba04/gender_age_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = (\n",
    "    data0\n",
    "    .filter(\"gender <> '-' OR age <> '-'\")\n",
    "    .withColumn(\"target\", F.concat(F.col(\"gender\"), F.col(\"age\")))\n",
    "    .drop(\"gender\", \"age\")\n",
    "    .withColumn(\"parse_json\", F.from_json(\"user_json\", MapType(StringType(), ArrayType(MapType(StringType(), StringType())))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data2 = (\n",
    "    data1\n",
    "    .withColumn(\"test\", data1.parse_json.visits)\n",
    "    .drop(\"user_json\", \"parse_json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = (\n",
    "    data2\n",
    "    .withColumn(\"urls\", F.expr('transform(test, x -> x.url)'))\n",
    "    .drop(\"test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data4 = (\n",
    "    data3\n",
    "    .withColumn(\"urls\", F.expr(\"transform(urls, x -> parse_url(x, 'HOST'))\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------+-------------------------------------------------------------------------------+\n",
      "|uid                                 |target|urls                                                                           |\n",
      "+------------------------------------+------+-------------------------------------------------------------------------------+\n",
      "|d50192e5-c44e-4ae8-ae7a-7cfe67c8b777|F18-24|[zebra-zoya.ru, news.yandex.ru, www.sotovik.ru, news.yandex.ru, www.sotovik.ru]|\n",
      "+------------------------------------+------+-------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data4.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kafka batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ravil.badamshin\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_batch0 = spark.read.format(\"kafka\").options(**batch_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_batch1 = (\n",
    "    kafka_batch0\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"string\"))\n",
    "    .select(\"value\")\n",
    "    .withColumn(\"uid\", F.get_json_object(\"value\", \"$.uid\"))\n",
    "    .withColumn(\"visits\", F.get_json_object(\"value\", \"$.visits\"))\n",
    "    .drop(\"value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_batch2 = (\n",
    "    kafka_batch1\n",
    "    .withColumn(\"parse_visits\", F.from_json(\"visits\", ArrayType(MapType(StringType(), StringType()))))\n",
    "    .drop(\"visits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_batch3 = (\n",
    "    kafka_batch2\n",
    "    .withColumn(\"urls\", F.expr('transform(parse_visits, x -> x.url)'))\n",
    "    .drop(\"parse_visits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kafka_batch4 = (\n",
    "    kafka_batch3\n",
    "    .withColumn(\"urls\", F.expr(\"transform(urls, x -> parse_url(x, 'HOST'))\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_indexer = StringIndexer(inputCol=\"target\", outputCol=\"targetIndex\")\n",
    "target_indexer.setHandleInvalid(\"keep\")\n",
    "target_indexer_fit = target_indexer.fit(data4)\n",
    "target_indexed = target_indexer_fit.transform(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_data, test_data) = target_indexed.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"targetIndex\", featuresCol=\"urls_vector\", numTrees=8, maxBins=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetConverter = IndexToString(inputCol=\"prediction\", \n",
    "                                outputCol=\"predicted_target\",\n",
    "                                labels=target_indexer_fit.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "urls_count_vectorizer = CountVectorizer(inputCol=\"urls\", outputCol=\"urls_vector\", vocabSize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    urls_count_vectorizer,\n",
    "    rf,\n",
    "    targetConverter\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+-----------+--------------------+--------------------+--------------------+----------+----------------+\n",
      "|                 uid|target|                urls|targetIndex|         urls_vector|       rawPrediction|         probability|prediction|predicted_target|\n",
      "+--------------------+------+--------------------+-----------+--------------------+--------------------+--------------------+----------+----------------+\n",
      "|05031311-38ba-46b...|M45-54|[frolnews.blogspo...|        6.0|(10000,[130,201,2...|[1.99684071322843...|[0.24960508915355...|       0.0|          M25-34|\n",
      "|05060864-cd46-48d...|M35-44|      [www.mnogo.ru]|        2.0|  (10000,[65],[1.0])|[1.92095459613138...|[0.24011932451642...|       0.0|          M25-34|\n",
      "|0507c191-2ed5-47d...|F45-54|[playcast.ru, www...|        5.0|(10000,[530,2336]...|[1.76507591767248...|[0.22063448970906...|       0.0|          M25-34|\n",
      "|050a33e2-e45e-4b7...|M25-34|[agligator.ru, ag...|        0.0|(10000,[3,9,10,36...|[1.98494640021193...|[0.24811830002649...|       0.0|          M25-34|\n",
      "|0512edd0-b787-417...|M35-44|[www.ignio.com, w...|        2.0|(10000,[121,201,6...|[1.92095459613138...|[0.24011932451642...|       0.0|          M25-34|\n",
      "+--------------------+------+--------------------+-----------+--------------------+--------------------+--------------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------------+\n",
      "|predicted_target|targetIndex|         urls_vector|\n",
      "+----------------+-----------+--------------------+\n",
      "|          M25-34|        6.0|(10000,[130,201,2...|\n",
      "|          M25-34|        2.0|  (10000,[65],[1.0])|\n",
      "|          M25-34|        5.0|(10000,[530,2336]...|\n",
      "|          M25-34|        0.0|(10000,[3,9,10,36...|\n",
      "|          M25-34|        2.0|(10000,[121,201,6...|\n",
      "+----------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"predicted_target\", \"targetIndex\", \"urls_vector\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.758423\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"targetIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(model.transform(test_data))\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/user/ravil.badamshin/model4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = PipelineModel.load(\"/user/ravil.badamshin/model4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_predictions = saved_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------------+\n",
      "|predicted_target|targetIndex|         urls_vector|\n",
      "+----------------+-----------+--------------------+\n",
      "|          M25-34|        6.0|(10000,[130,201,2...|\n",
      "|          M25-34|        2.0|  (10000,[65],[1.0])|\n",
      "|          M25-34|        5.0|(10000,[530,2336]...|\n",
      "|          M25-34|        0.0|(10000,[3,9,10,36...|\n",
      "|          M25-34|        2.0|(10000,[121,201,6...|\n",
      "+----------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_predictions.select(\"predicted_target\", \"targetIndex\", \"urls_vector\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.758423\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(saved_predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on kafka batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions = saved_model.transform(kafka_batch4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions = (\n",
    "    batch_predictions\n",
    "    .select(\"*\", F.col(\"predicted_target\").substr(1, 1).alias(\"gender\"), F.col(\"predicted_target\").substr(2, 5).alias(\"age\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+----------------+------+-----+\n",
      "|                 uid|                urls|         urls_vector|       rawPrediction|         probability|prediction|predicted_target|gender|  age|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+----------------+------+-----+\n",
      "|bd7a30e1-a25d-4cb...|[www.interfax.ru,...|(10000,[0,3,6,9,1...|[2.12060761675731...|[0.26507595209466...|       0.0|          M25-34|     M|25-34|\n",
      "|bd7a6f52-45db-49b...|[www.packagetrack...|(10000,[0,3,9,10,...|[1.97377440285684...|[0.24672180035710...|       0.0|          M25-34|     M|25-34|\n",
      "|bd7a7fd9-ab06-42f...|[www.mk.ru, www.m...|(10000,[78,1279],...|[1.92095459613138...|[0.24011932451642...|       0.0|          M25-34|     M|25-34|\n",
      "|bd7c5d7a-0def-41d...|[www.24open.ru, w...|(10000,[0,3,6,39,...|[1.87514658062919...|[0.23439332257864...|       0.0|          M25-34|     M|25-34|\n",
      "|bd7e54a2-0215-45c...|[www.dns-shop.ru,...|(10000,[74,2277],...|[1.92095459613138...|[0.24011932451642...|       0.0|          M25-34|     M|25-34|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+----------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|                 uid|gender|  age|\n",
      "+--------------------+------+-----+\n",
      "|bd7a30e1-a25d-4cb...|     M|25-34|\n",
      "|bd7a6f52-45db-49b...|     M|25-34|\n",
      "|bd7a7fd9-ab06-42f...|     M|25-34|\n",
      "|bd7c5d7a-0def-41d...|     M|25-34|\n",
      "|bd7e54a2-0215-45c...|     M|25-34|\n",
      "+--------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_predictions.select(\"uid\", \"gender\", \"age\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0|252654|\n",
      "|       1.0|  1530|\n",
      "|       3.0|    51|\n",
      "|       2.0|   765|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_predictions.select(\"uid\", \"prediction\").groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kafka streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ravil.badamshin\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"ravil.badamshin\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = SparkSession.builder.getOrCreate().streams.active\n",
    "if streams:\n",
    "    for s in streams:\n",
    "        desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "        s.stop()\n",
    "        print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7fc8b5efb160>]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkSession.builder.getOrCreate().streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf0 = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf1 = (\n",
    "    kafka_sdf0\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"string\"))\n",
    "    .select(\"value\")\n",
    "    .withColumn(\"uid\", F.get_json_object(\"value\", \"$.uid\"))\n",
    "    .withColumn(\"visits\", F.get_json_object(\"value\", \"$.visits\"))\n",
    "    .drop(\"value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf2 = (\n",
    "    kafka_sdf1\n",
    "    .withColumn(\"parse_visits\", F.from_json(\"visits\", ArrayType(MapType(StringType(), StringType()))))\n",
    "    .drop(\"visits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf3 = (\n",
    "    kafka_sdf2\n",
    "    .withColumn(\"urls\", F.expr('transform(parse_visits, x -> x.url)'))\n",
    "    .drop(\"parse_visits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf4 = (\n",
    "    kafka_sdf3\n",
    "    .withColumn(\"urls\", F.expr(\"transform(urls, x -> parse_url(x, 'HOST'))\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_predictions0 = model.transform(kafka_sdf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_predictions1 = (\n",
    "    kafka_predictions0\n",
    "    .select(\"uid\", \n",
    "            F.col(\"predicted_target\").substr(1, 1).alias(\"gender\"), \n",
    "            F.col(\"predicted_target\").substr(2, 5).alias(\"age\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_out_df = (\n",
    "    kafka_predictions1\n",
    "    .select(F.to_json(F.struct(*kafka_predictions1.columns)).alias(\"value\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fc8b5e6f198>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kafka_out_df\n",
    " .writeStream.format(\"kafka\").options(**write_kafka_params)\n",
    " .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\n",
    " .outputMode(\"append\")\n",
    " .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
