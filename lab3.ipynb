{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 2g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4057\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7803f5048>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, HashingTF\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"purchase\", IntegerType(), True),\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", schema=schema, header=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "|   1654|  66187|       0|\n",
      "|   1654|  84350|       0|\n",
      "|   1654|  92854|       0|\n",
      "|   1654|  72811|       0|\n",
      "|   1654|  86876|       0|\n",
      "|   1654| 102657|       0|\n",
      "|   1654| 100482|       0|\n",
      "|   1654|  89677|       0|\n",
      "|   1654|  99419|       0|\n",
      "|   1654|  66603|       0|\n",
      "|   1654|   7363|       0|\n",
      "|   1654|   1320|       0|\n",
      "|   1654|  88892|       0|\n",
      "|   1654|  66671|       0|\n",
      "|   1654|  75925|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", schema=schema, header=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|item_id|\n",
      "+-------+-------+\n",
      "|   1654|  94814|\n",
      "|   1654|  93629|\n",
      "|   1654|   9980|\n",
      "|   1654|  95099|\n",
      "|   1654|  11265|\n",
      "|   1654|  88896|\n",
      "|   1654|  67740|\n",
      "|   1654|  74271|\n",
      "|   1654|  99871|\n",
      "|   1654|  78570|\n",
      "|   1654|  71942|\n",
      "|   1654|  74367|\n",
      "|   1654|  98628|\n",
      "|   1654|  95887|\n",
      "|   1654|  77795|\n",
      "|   1654|  75152|\n",
      "|   1654|  74905|\n",
      "|   1654|   9068|\n",
      "|   1654|  72954|\n",
      "|   1654| 102431|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", IntegerType()),\n",
    "    StructField(\"datetime_availability_start\", StringType()),\n",
    "    StructField(\"datetime_availability_stop\", StringType()),\n",
    "    StructField(\"datetime_show_start\", StringType()),\n",
    "    StructField(\"datetime_show_stop\", StringType()),\n",
    "    StructField(\"content_type\", IntegerType()),\n",
    "    StructField(\"title\", StringType(), nullable=True),\n",
    "    StructField(\"year\", FloatType(), nullable=True),\n",
    "    StructField(\"genres\", StringType()),\n",
    "    StructField(\"region_id\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = spark.read.option('delimiter', '\\t').csv(\"/labs/slaba03/laba03_items.csv\", header=True, schema=items_schema, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------+------+--------------------+---------+\n",
      "|item_id|channel_id|datetime_availability_start|datetime_availability_stop|datetime_show_start|datetime_show_stop|content_type|               title|  year|              genres|region_id|\n",
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------+------+--------------------+---------+\n",
      "|  65667|      null|       1970-01-01T00:00:00Z|      2018-01-01T00:00:00Z|               null|              null|           1|на пробах только ...|2013.0|             Эротика|     null|\n",
      "|  65669|      null|       1970-01-01T00:00:00Z|      2018-01-01T00:00:00Z|               null|              null|           1|скуби ду: эротиче...|2011.0|             Эротика|     null|\n",
      "|  65668|      null|       1970-01-01T00:00:00Z|      2018-01-01T00:00:00Z|               null|              null|           1|горячие девочки д...|2011.0|             Эротика|     null|\n",
      "|  65671|      null|       1970-01-01T00:00:00Z|      2018-01-01T00:00:00Z|               null|              null|           1|соблазнительницы ...|2011.0|             Эротика|     null|\n",
      "|  65670|      null|       1970-01-01T00:00:00Z|      2018-01-01T00:00:00Z|               null|              null|           1|секретные секс-ма...|2010.0|             Эротика|     null|\n",
      "|  65809|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|      все о мужчинах|2016.0|             Комедии|     null|\n",
      "|  65810|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|8 лучших свиданий...|2016.0|   Комедии,Мелодрамы|     null|\n",
      "|    326|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|            византия|2012.0|Ужасы,Триллеры,Др...|     null|\n",
      "|    336|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|девственники, бер...|2012.0|Ужасы,Комедии,Фан...|     null|\n",
      "|    357|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|   8 первых свиданий|2012.0|Комедии,Мелодрамы...|     null|\n",
      "|    396|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|        предчувствие|2007.0|Детективы,Триллер...|     null|\n",
      "|    400|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|          путь воина|2010.0|Фантастика,Боевик...|     null|\n",
      "|    423|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|    список контактов|2008.0|Детективы,Триллер...|     null|\n",
      "|    430|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|               сыщик|2007.0|Детективы,Триллер...|     null|\n",
      "|    449|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|             туристы|2006.0|Ужасы,Детективы,Т...|     null|\n",
      "|    453|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|      уроки вождения|2006.0|Комедии,Драмы,Зар...|     null|\n",
      "|    478|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|             апостол|2008.0|Военные,Боевики,Наши|     null|\n",
      "|    495|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|     железный рыцарь|2010.0|Приключения,Истор...|     null|\n",
      "|    505|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|че: часть первая....|2008.0|Военные,Драмы,Ист...|     null|\n",
      "|    540|      null|       1970-01-01T00:00:00Z|      2099-12-31T21:00:00Z|               null|              null|           1|   мой парень – псих|2012.0|Комедии,Драмы,Мел...|     null|\n",
      "+-------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------+------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.filter(F.col(\"content_type\") == 1)\n",
    "df_items = df_items.select(F.regexp_replace('title', r',|\\.|&|\\\\|\\||–|_|:|-|\\!|\\?', '').alias('title'), 'item_id', 'year', 'genres')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, when, array\n",
    "df_items2 = df_items.withColumn(\"genres_word\", split(\"genres\", \",\"))\n",
    "df_items2  = df_items2.withColumn('genres_word', when(df_items2['genres_word'].isNull(), array().cast(\"array<integer>\")).otherwise(df_items2['genres_word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+--------------------+--------------------+\n",
      "|               title|item_id|  year|              genres|         genres_word|\n",
      "+--------------------+-------+------+--------------------+--------------------+\n",
      "|на пробах только ...|  65667|2013.0|             Эротика|           [Эротика]|\n",
      "|скуби ду эротичес...|  65669|2011.0|             Эротика|           [Эротика]|\n",
      "|горячие девочки д...|  65668|2011.0|             Эротика|           [Эротика]|\n",
      "|соблазнительницы ...|  65671|2011.0|             Эротика|           [Эротика]|\n",
      "|секретные сексмат...|  65670|2010.0|             Эротика|           [Эротика]|\n",
      "|      все о мужчинах|  65809|2016.0|             Комедии|           [Комедии]|\n",
      "|8 лучших свиданий...|  65810|2016.0|   Комедии,Мелодрамы|[Комедии, Мелодрамы]|\n",
      "|            византия|    326|2012.0|Ужасы,Триллеры,Др...|[Ужасы, Триллеры,...|\n",
      "|девственники бере...|    336|2012.0|Ужасы,Комедии,Фан...|[Ужасы, Комедии, ...|\n",
      "|   8 первых свиданий|    357|2012.0|Комедии,Мелодрамы...|[Комедии, Мелодра...|\n",
      "|        предчувствие|    396|2007.0|Детективы,Триллер...|[Детективы, Трилл...|\n",
      "|          путь воина|    400|2010.0|Фантастика,Боевик...|[Фантастика, Боев...|\n",
      "|    список контактов|    423|2008.0|Детективы,Триллер...|[Детективы, Трилл...|\n",
      "|               сыщик|    430|2007.0|Детективы,Триллер...|[Детективы, Трилл...|\n",
      "|             туристы|    449|2006.0|Ужасы,Детективы,Т...|[Ужасы, Детективы...|\n",
      "|      уроки вождения|    453|2006.0|Комедии,Драмы,Зар...|[Комедии, Драмы, ...|\n",
      "|             апостол|    478|2008.0|Военные,Боевики,Наши|[Военные, Боевики...|\n",
      "|     железный рыцарь|    495|2010.0|Приключения,Истор...|[Приключения, Ист...|\n",
      "|че часть первая а...|    505|2008.0|Военные,Драмы,Ист...|[Военные, Драмы, ...|\n",
      "|    мой парень  псих|    540|2012.0|Комедии,Драмы,Мел...|[Комедии, Драмы, ...|\n",
      "+--------------------+-------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"russian\")  \n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"title_words_filtered\", stopWords=stop_words)\n",
    "#count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"word_vector\", vocabSize=10000)\n",
    "#tfidf = IDF(inputCol=\"word_vector\", outputCol=\"tfidf\")    \n",
    "\n",
    "hasher = HashingTF(numFeatures=200, binary=True, inputCol=swr.getOutputCol(), outputCol=\"word_vector\")\n",
    "\n",
    "\n",
    "preprocessing = Pipeline(stages=[\n",
    "        tokenizer,\n",
    "        swr,\n",
    "        #count_vectorizer,\n",
    "        #tfidf\n",
    "        hasher\n",
    "    ])\n",
    "    \n",
    "preprocessing_model = preprocessing.fit(df_items2)\n",
    "df_items3 = preprocessing_model.transform(df_items2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " title                | на пробах только ... \n",
      " item_id              | 65667                \n",
      " year                 | 2013.0               \n",
      " genres               | Эротика              \n",
      " genres_word          | [Эротика]            \n",
      " title_words          | [на, пробах, толь... \n",
      " title_words_filtered | [пробах, девушки,... \n",
      " word_vector          | (200,[0,4,23,29,8... \n",
      "-RECORD 1------------------------------------\n",
      " title                | скуби ду эротичес... \n",
      " item_id              | 65669                \n",
      " year                 | 2011.0               \n",
      " genres               | Эротика              \n",
      " genres_word          | [Эротика]            \n",
      " title_words          | [скуби, ду, эроти... \n",
      " title_words_filtered | [скуби, ду, эроти... \n",
      " word_vector          | (200,[17,33,99,10... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items3.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol='genres_word', outputCol='genre_vector')\n",
    "items = count_vectorizer.fit(df_items3).transform(df_items3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               title|item_id|  year|              genres|         genres_word|         title_words|title_words_filtered|         word_vector|        genre_vector|\n",
      "+--------------------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|на пробах только ...|  65667|2013.0|             Эротика|           [Эротика]|[на, пробах, толь...|[пробах, девушки,...|(200,[0,4,23,29,8...|     (83,[20],[1.0])|\n",
      "|скуби ду эротичес...|  65669|2011.0|             Эротика|           [Эротика]|[скуби, ду, эроти...|[скуби, ду, эроти...|(200,[17,33,99,10...|     (83,[20],[1.0])|\n",
      "|горячие девочки д...|  65668|2011.0|             Эротика|           [Эротика]|[горячие, девочки...|[горячие, девочки...|(200,[31,51,67,93...|     (83,[20],[1.0])|\n",
      "|соблазнительницы ...|  65671|2011.0|             Эротика|           [Эротика]|[соблазнительницы...|[соблазнительницы...|(200,[66,131,153,...|     (83,[20],[1.0])|\n",
      "|секретные сексмат...|  65670|2010.0|             Эротика|           [Эротика]|[секретные, сексм...|[секретные, сексм...|(200,[49,52,63,66...|     (83,[20],[1.0])|\n",
      "|      все о мужчинах|  65809|2016.0|             Комедии|           [Комедии]|  [все, о, мужчинах]|          [мужчинах]|   (200,[146],[1.0])|      (83,[2],[1.0])|\n",
      "|8 лучших свиданий...|  65810|2016.0|   Комедии,Мелодрамы|[Комедии, Мелодрамы]|[8, лучших, свида...|[8, лучших, свида...|(200,[26,160,180,...|(83,[2,7],[1.0,1.0])|\n",
      "|            византия|    326|2012.0|Ужасы,Триллеры,Др...|[Ужасы, Триллеры,...|          [византия]|          [византия]|   (200,[137],[1.0])|(83,[0,1,3,10,12]...|\n",
      "|девственники бере...|    336|2012.0|Ужасы,Комедии,Фан...|[Ужасы, Комедии, ...|[девственники, бе...|[девственники, бе...|(200,[143,190],[1...|(83,[0,2,10,12],[...|\n",
      "|   8 первых свиданий|    357|2012.0|Комедии,Мелодрамы...|[Комедии, Мелодра...|[8, первых, свида...|[8, первых, свида...|(200,[26,82,180],...|(83,[2,6,7],[1.0,...|\n",
      "|        предчувствие|    396|2007.0|Детективы,Триллер...|[Детективы, Трилл...|      [предчувствие]|      [предчувствие]|   (200,[138],[1.0])|(83,[0,1,3,11,12]...|\n",
      "|          путь воина|    400|2010.0|Фантастика,Боевик...|[Фантастика, Боев...|       [путь, воина]|       [путь, воина]|(200,[142,183],[1...|(83,[0,5,12],[1.0...|\n",
      "|    список контактов|    423|2008.0|Детективы,Триллер...|[Детективы, Трилл...| [список, контактов]| [список, контактов]|(200,[0,163],[1.0...|(83,[0,1,3,11,13]...|\n",
      "|               сыщик|    430|2007.0|Детективы,Триллер...|[Детективы, Трилл...|             [сыщик]|             [сыщик]|    (200,[73],[1.0])|(83,[0,1,3,11],[1...|\n",
      "|             туристы|    449|2006.0|Ужасы,Детективы,Т...|[Ужасы, Детективы...|           [туристы]|           [туристы]|   (200,[110],[1.0])|(83,[0,3,10,11],[...|\n",
      "|      уроки вождения|    453|2006.0|Комедии,Драмы,Зар...|[Комедии, Драмы, ...|   [уроки, вождения]|   [уроки, вождения]|(200,[21,147],[1....|(83,[0,1,2],[1.0,...|\n",
      "|             апостол|    478|2008.0|Военные,Боевики,Наши|[Военные, Боевики...|           [апостол]|           [апостол]|    (200,[67],[1.0])|(83,[5,6,16],[1.0...|\n",
      "|     железный рыцарь|    495|2010.0|Приключения,Истор...|[Приключения, Ист...|  [железный, рыцарь]|  [железный, рыцарь]|(200,[88,177],[1....|(83,[0,5,7,8,25],...|\n",
      "|че часть первая а...|    505|2008.0|Военные,Драмы,Ист...|[Военные, Драмы, ...|[че, часть, перва...|[че, часть, перва...|(200,[14,34,122],...|(83,[0,1,16,25],[...|\n",
      "|    мой парень  псих|    540|2012.0|Комедии,Драмы,Мел...|[Комедии, Драмы, ...|[мой, парень, , п...|    [парень, , псих]|(200,[44,164,172]...|(83,[0,1,2,7],[1....|\n",
      "+--------------------+-------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# обьединение фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['word_vector', 'genre_vector'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = assembler.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 520446               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 1------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 556825               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 2------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 566701               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 3------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 613775               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 4------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 619378               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 5------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 625678               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 6------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 632495               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 7------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 636572               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 8------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 639612               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 9------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 668112               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 10-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 703514               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 11-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 711308               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 12-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 717302               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 13-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 719149               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 14-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 725821               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 15-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 728960               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 16-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 729785               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 17-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 731490               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 18-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 732411               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "-RECORD 19-----------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 735387               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      " features             | (283,[58,84,187,2... \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import MinMaxScaler\n",
    "# scaler = MinMaxScaler(inputCol='features', outputCol='features_minmax')\n",
    "# scaler_model = scaler.fit(items)\n",
    "# items1 = scaler_model.transform(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# обьединяем train и items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_user.join(items, on=\"item_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 520446               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      "-RECORD 1------------------------------------\n",
      " item_id              | 8389                 \n",
      " user_id              | 556825               \n",
      " purchase             | 0                    \n",
      " title                | пес в сапогах (су... \n",
      " year                 | 1981.0               \n",
      " genres               | Мультфильмы,Детск... \n",
      " genres_word          | [Мультфильмы, Дет... \n",
      " title_words          | [пес, в, сапогах,... \n",
      " title_words_filtered | [пес, сапогах, (с... \n",
      " word_vector          | (200,[58,84,187],... \n",
      " genre_vector         | (83,[6,14,19,23],... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(2,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# разбиваем на train и val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df_train.join(train, on=\"user_id\", how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol=\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1275.fit.\n: org.apache.spark.SparkException: Job 105 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:330)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:206)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-953e2e35f21b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1275.fit.\n: org.apache.spark.SparkException: Job 105 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:330)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:206)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
